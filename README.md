# HOI-R1: Exploring the Potential of Multimodal Large Language Models for Human-Object Interaction Detection

[![arXiv](https://img.shields.io/badge/arXiv-2510.05609-b31b1b.svg)](https://arxiv.org/abs/2510.05609)
[![Hugging Face](https://img.shields.io/badge/ü§ó%20HuggingFace-Model-yellow)](https://huggingface.co/thxplz/HOI-R1_Qwen2.5-VL-3B-Instruct)

This repository contains the official resources for **HOI-R1**, a research project that explores the potential of **Multimodal Large Language Models (MLLMs)** for **Human-Object Interaction (HOI) Detection**.

HOI-R1 is inspired by recent advances in reinforcement learning for large language models and investigates how vision-language models can reason about and detect human-object interactions more effectively.

---

## üîç Overview

- **Task**: Human-Object Interaction Detection (HOID)
- **Our Motivation**:  
  Leverage the reasoning capability of Multimodal LLMs and reinforcement learning‚Äìstyle optimization to explore HOI detection performance.

![hoi-r1-arch](https://cdn-uploads.huggingface.co/production/uploads/63119ce2fb65b9a3e2f75e3c/tHYWwrnqBAHsoo8lIOtnM.jpeg)

---

## ü§ó Model Weights

Model checkpoints are available on Hugging Face:

üëâ **HOI-R1_Qwen2.5-VL-3B-Instruct**  
https://huggingface.co/thxplz/HOI-R1_Qwen2.5-VL-3B-Instruct

---

## üõ† TODO

- [ ] Add qualitative visualization examples
- [ ] Release inference & evaluation scripts
- [ ] Add dataset preprocessing pipeline
- [ ] Provide detailed training configuration and hyperparameters  
- [ ] Release training code
- [ ] Release additional model variants

---

## üìå Citation

If you find this work useful, please consider citing:

```bibtex
@article{chen2025hoi,
  title={HOI-R1: Exploring the Potential of Multimodal Large Language Models for Human-Object Interaction Detection},
  author={Chen, Junwen and Xiong, Peilin and Yanai, Keiji},
  journal={arXiv preprint arXiv:2510.05609},
  year={2025}
}
